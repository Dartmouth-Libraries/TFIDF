{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a121e8a",
   "metadata": {},
   "source": [
    "# Text Analysis in Python 3: Comparing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66fec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1005d848",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Data Frequency (TFIDF)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/720/1*qQgnyPLDIkUmeZKN2_ZWbQ.webp\" style=\"width:60%\">\n",
    "\n",
    "Image from Yassine Hamdaoui, [\"TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python\"](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558) *Towards Data Science (Medium)* (Dec. 9, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e35663",
   "metadata": {},
   "source": [
    "For this lesson, we will be drawing on the [TFIDF section](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/01-TF-IDF.html) in the online book: Melanie Walsh, [*Introduction to Cultural Analytics and Python*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html), Version 1 (2021), https://doi.org/10.5281/zenodo.4411250. \n",
    "\n",
    "<img src=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/_static/favicon.ico\" style=\"width:30%\">\n",
    "\n",
    "All sections below labeled with a **MW** comes from this book. Please consider supporting that project if you find it useful.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2745b",
   "metadata": {},
   "source": [
    "## TF-IDF with Scikit-Learn [MW]\n",
    "\n",
    "Tf-idf is a method that tries to identify the most distinctively frequent or significant words in a document. \n",
    "\n",
    "In this lesson, we’re going to learn how to calculate tf-idf scores using a collection of plain text (.txt) files and the Python library scikit-learn, which has a quick and nifty module called TfidfVectorizer.\n",
    "\n",
    "In this lesson, we will cover how to:\n",
    "\n",
    "    Calculate and normalize tf-idf scores for U.S. Inaugural Addresses with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e7f59",
   "metadata": {},
   "source": [
    "## Dataset: U.S. Inaugural Addresses [MW]\n",
    "\n",
    "    This is the meaning of our liberty and our creed; why men and women and children of every race and every faith can join in celebration across this magnificent Mall, and why a man whose father less than 60 years ago might not have been served at a local restaurant can now stand before you to take a most sacred oath. So let us mark this day with remembrance of who we are and how far we have traveled.\n",
    "\n",
    "    —Barack Obama, Inaugural Presidential Address, January 2009\n",
    "\n",
    "During Barack Obama’s Inaugural Address in January 2009, he mentioned “women” four different times, including in the passage quoted above. How distinctive is Obama’s inclusion of women in this address compared to all other U.S. Presidents? This is one of the questions that we’re going to try to answer with tf-idf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4897fbb",
   "metadata": {},
   "source": [
    "## Breaking Down the TF-IDF Formula [MW]\n",
    "\n",
    "But first, let’s quickly discuss the tf-idf formula. The idea is pretty simple.\n",
    "\n",
    "**tf-idf = term_frequency * inverse_document_frequency**\n",
    "\n",
    "**term_frequency** = number of times a given term appears in document\n",
    "\n",
    "**inverse_document_frequency** = log(total number of documents / number of documents with term) + 1*****\n",
    "\n",
    "You take the number of times a term occurs in a document (term frequency). Then you take the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and you flip that fraction on its head (inverse document frequency). Then you multiply the two numbers together (term_frequency * inverse_document_frequency).\n",
    "\n",
    "The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents. Think about the inverse document frequency for the word “said” vs the word “pigeon.” The term “said” appears in 13 (document frequency) of 14 (total documents) Lost in the City stories (14 / 13 –> a smaller inverse document frequency) while the term “pigeons” only occurs in 2 (document frequency) of the 14 stories (total documents) (14 / 2 –> a bigger inverse document frequency, a bigger tf-idf boost).\n",
    "\n",
    "*There are a bunch of slightly different ways that you can calculate inverse document frequency. The version of idf that we’re going to use is the scikit-learn default, which uses “smoothing” aka it adds a “1” to the numerator and denominator:\n",
    "\n",
    "**inverse_document_frequency** = log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bfe6bd9",
   "metadata": {},
   "source": [
    "## Part I: TF-IDF with scikit-learn [MW]\n",
    "\n",
    "*Additional comments by Jeremy denoted by [jm]*\n",
    "\n",
    "scikit-learn, imported as sklearn, is a popular Python library for machine learning approaches such as clustering, classification, and regression. Though we’re not doing any machine learning in this lesson, we’re nevertheless going to use scikit-learn’s TfidfVectorizer and CountVectorizer.\n",
    "\n",
    "<p><del>1. Install scikit-learn</del></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "#already installed on JHub!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6af03ea2",
   "metadata": {},
   "source": [
    "2. Import necessary modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4124907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')                           #[jm] remember, this is the tokenizer that removes punctuation\n",
    "import pandas as pd, numpy as np, altair as alt\n",
    "# pd.set_option(\"max_rows\",600)                            # [jm] returns an OptionError\n",
    "pd.set_option(\"display.max_rows\",600)                      #[jm] apparently max_rows commands has now been replaced w/ display.max_rows\n",
    "from pathlib import Path\n",
    "import glob, collections\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09c3b5",
   "metadata": {},
   "source": [
    "We’re also going to import pandas and change its default display setting. And we’re going to import two libraries that will help us work with files and the file system: [pathlib](https://docs.python.org/3/library/pathlib.html##basic-use) and [glob](https://docs.python.org/3/library/glob.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d871c33",
   "metadata": {},
   "source": [
    "3. Below we’re setting the directory filepath that contains all the text files that we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41763a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"US_Inaugural_Addresses\"\n",
    "#MW: Then we’re going to use glob and Path to make a list of all the filepaths in that directory and a list of all the short story titles.\n",
    "#text_files = glob.glob(f\"../../RR/shared/{directory_path}/*.txt\")\n",
    "inaugdir = Path(Path.home(), \"shared\", \"RR-workshop-data\", f\"{directory_path}\")\n",
    "text_files = glob.glob(f\"{inaugdir}/*.txt\")\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles = [Path(text).stem for text in text_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74380f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/F0040RP')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"~/shared/RR-workshops-data/US_Inaugural_Addresses\").expanduser() \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab0de64",
   "metadata": {},
   "source": [
    "## Part II. Calculate tf-idf [MW]\n",
    "\n",
    "To calculate tf–idf scores for every word, we’re going to use scikit-learn’s TfidfVectorizer.\n",
    "\n",
    "4. When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
    "\n",
    "The recommended way to run TfidfVectorizer is with smoothing (smooth_idf = True) and normalization (norm='l2') turned on. These parameters will better account for differences in text length, and overall produce more meaningful tf–idf scores. Smoothing and L2 normalization are actually the default settings for TfidfVectorizer, so to turn them on, you don’t need to include any extra code at all.\n",
    "\n",
    "Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "227cb304",
   "metadata": {},
   "source": [
    "5. Run TfidfVectorizer on our text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47366550",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f70858e",
   "metadata": {},
   "source": [
    "6. Make a DataFrame out of the resulting tf–idf vector, setting the “feature names” or words as columns and the titles as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note from Simon: TfidfVectorizer returns a sparse matrix and that's why we have to call .toarray()  before proceeding.\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#warning: get_feature_names will be depreciated; use get_feature_names_out instead\n",
    "   ##I made this fix in the code above\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb79898c",
   "metadata": {},
   "source": [
    "## Part III. Practice with Dataframes [jm]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebd8834d",
   "metadata": {},
   "source": [
    "### Summary Info\n",
    "\n",
    "7. Often, when working with dataframes in Python, it is helpful to get a quick overview of the type, size, and distribution of the data it contains.\n",
    "\n",
    "Run the following commands and add a comment next to each explaining what it does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbaf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head(3)   ## [explain what this does here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9beb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8098af42",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "\n",
    "8. We can also create smaller subsets of a dataframe in a variety of ways. Run the following code and explain what each does in a comment next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ada1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tfidf_df[['women']]       # [explain what this does]\n",
    "sub.tail(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = tfidf_df[['freedom','liberty','democracy']]\n",
    "sub = tfidf_df.loc[:, ['freedom','liberty','democracy']]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tfidf_df.loc[\"02_washington_1793\":\"05_jefferson_1805\",[\"war\", \"economy\"]]\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f403c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tfidf_df.iloc[55:58,:]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tfidf_df.iloc[:,1000:1005]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ee9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tfidf_df.iloc[[10,20,30,40,50],[-1,-2,-3,-4]]\n",
    "sub.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb29b2f7",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "9. We can also filter dataframes by the values found in columns or rows. Review what the following code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = tfidf_df[tfidf_df['democracy']>0.1]      # keeps only those rows with a frequency greater than 0.1 in the democracy column\n",
    "filterdf.head()                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1520a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = tfidf_df.loc[tfidf_df['democracy']>0.1, 'democracy']\n",
    "filterdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dad348",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterdf = tfidf_df.loc[(tfidf_df['democracy']>0.05) & (tfidf_df['freedom']>0.03), ['democracy','freedom']]\n",
    "filterdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['god'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ae8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb76cb22",
   "metadata": {},
   "source": [
    "## Part IV. Subsetting, Filtering, and Sorting the TFIDF Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ce30590",
   "metadata": {},
   "source": [
    "10. Add column for document frequency aka number of times word appears in all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ef2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.loc['00_DocumentFrequency'] = (tfidf_df > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_slice = tfidf_df[['government', 'borders', 'people', 'obama', 'war', 'honor','foreign', 'men', 'women', 'children']]\n",
    "tfidf_slice.sort_index().round(decimals=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "215a46d6",
   "metadata": {},
   "source": [
    "11. Let’s drop “OO_Document Frequency” since we were just using it for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.drop('00_DocumentFrequency', errors='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37792161",
   "metadata": {},
   "source": [
    "12. Let’s reorganize the DataFrame so that the words are in rows rather than columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3574582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da636fe5",
   "metadata": {},
   "source": [
    "13. To find out the top 10 words with the highest tf–idf for every story, we’re going to sort by document and tfidf score and then groupby document and take the first 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8383cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b90431e",
   "metadata": {},
   "source": [
    "14. We can zoom in on particular words and particular documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737113fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf[top_tfidf['term'].str.contains('women')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf['term'].str.contains('e')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da810a65",
   "metadata": {},
   "source": [
    "15. It turns out that the term “women” is very distinctive in Obama’s Inaugural Address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf[top_tfidf['document'].str.contains('obama')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf[top_tfidf['document'].str.contains('trump')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4404112",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf[top_tfidf['document'].str.contains('kennedy')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4f759a7",
   "metadata": {},
   "source": [
    "## Visualize TF-IDF [MW]\n",
    "\n",
    "17. We can also visualize our TF-IDF results with the data visualization library Altair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef845a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install altair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d36f2c16",
   "metadata": {},
   "source": [
    "18. Let’s make a heatmap that shows the highest TF-IDF scoring words for each president, and let’s put a red dot next to two terms of interest: “war” and “peace”:\n",
    "\n",
    "The code below was contributed by [Eric Monson](https://github.com/emonson). Thanks, Eric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa371b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "# Terms in this list will get a red dot in the visualization\n",
    "term_list = ['war', 'peace']\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_tfidf.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# red circle over terms in above list\n",
    "circle = base.mark_circle(size=100).encode(\n",
    "    color = alt.condition(\n",
    "        alt.FieldOneOfPredicate(field='term', oneOf=term_list),\n",
    "        alt.value('red'),\n",
    "        alt.value('#FFFFFF00')        \n",
    "    )\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + circle + text).properties(width = 600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9ebbfe6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><h3 style='color:blue'> Review Questions </h3>\n",
    "\n",
    "<p style='color:blue'>Your Turn!</p>\n",
    "\n",
    "<p style='color:blue'>Take a few minutes to explore the dataframe below and then answer the following questions.</p>\n",
    "\n",
    "<p style='color:blue'>1. What is the difference between a tf-idf score and raw word frequency?</p>\n",
    "\n",
    "<p style='color:blue'>2. Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?</p>\n",
    "\n",
    "<p style='color:blue'>3. What’s another collection of texts that you think might be interesting to analyze with tf-idf scores? Why?</p>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2fc592",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"><h3 style='color:blue'> EXERCISES </h3>\n",
    "\n",
    "<p style='color:blue'>Write new code below that creates a TFIDF dataframe and color-coded graphic (just like the one above) but this time for either:</p>\n",
    "\n",
    "<ol style='color:blue'>\n",
    "    <li>our 233 SOTU addresses</li>\n",
    "    <li>or a collection of texts of your own choosing (which should be saved as plain text [.txt] files)</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "92ccb6e3f8e1ba46ac70611fe300a00d231540f34c9f821035b67580ebdf166e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
